{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ViVa-SAFELAND: Dataset Creation Tool","text":"<p>ViVa-SAFELAND is an open-source tool for creating semantic segmentation datasets by tracking objects of interest from videos. It leverages the SAM 2 (Segment Anything Model 2) and YOLO AI models to perform segmentation and object detection, guiding users through an 8-step process to generate complete datasets ready for model training.</p> ViVa-SAFELAND: Graphical User Interface for Dataset Creation <p>This tool focuses on generating semantic segmentation datasets through object tracking, utilizing SAM 2 to enhance segmentation accuracy.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Video-to-Dataset Conversion: Transform videos into high-quality segmentation datasets with minimal manual effort.</li> <li>SAM 2 Integration: Utilize the latest Segment Anything Model 2 for accurate and interactive segmentation.</li> <li>8-Step Guided Process: Step-by-step workflow ensuring comprehensive dataset creation from frame extraction to final composition.</li> <li>Interactive Refinement: Manually refine segmentations for precision and quality control.</li> <li>Object Tracking Integration: Utilize YOLO and DeepSort for tracking objects of interest across video frames.</li> <li>Batch Processing: Efficiently handle large videos through configurable batch processing.</li> <li>Customizable Classes: Define and assign custom object classes with unique colors.</li> <li>Safety-Focused: Designed for safe and reliable dataset generation without hardware risks.</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>For detailed usage instructions, examples, and API documentation, please refer to the ViVa-DataCreator Documentation.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use ViVa-DataCreator in your research, please cite our work:</p> <pre><code>@software{soriano_garcia_viva_datacreator_2025,\n  author = {Miguel Soriano-Garc\u00eda, Diego Mercado-Ravell, Israel Becerra and Julio De La Torre-Vanegas},\n  title = {ViVa-DataCreator: Dataset Creation Tool},\n  year = {2025},\n  url = {https://github.com/viva-safeland/viva_datacreator}\n}\n</code></pre>"},{"location":"about/","title":"About","text":"<p>This work was supported by the Office of Naval Research Global ONRG, Award No. <code>N62909-24-1-2001</code>.</p>"},{"location":"about/#license","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2025 ViVa-SAFELAND\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"cli-reference/","title":"CLI Reference","text":"<p>This guide provides detailed information about all command-line interface (CLI) commands available in Segmented Creator, including their parameters and usage examples.</p>"},{"location":"cli-reference/#command-structure","title":"Command Structure","text":"<p>All commands follow the pattern: <pre><code>uv run python -m vivadatacreator.&lt;step_name&gt; [OPTIONS]\n</code></pre></p>"},{"location":"cli-reference/#common-parameters","title":"Common Parameters","text":"<p>Most steps share these common parameters:</p> <ul> <li><code>--root PATH</code>: Path to the video file (required)</li> <li><code>--fac INT</code>: Scaling factor for image resizing (default: 2)</li> <li><code>--sam2-chkpt PATH</code>: Path to SAM2 checkpoint file</li> <li><code>--model-cfg PATH</code>: Path to SAM2 model configuration file</li> <li><code>--n-imgs INT</code>: Number of images to process per batch (default: 200)</li> <li><code>--n-obj INT</code>: Number of objects to process simultaneously (default: 20)</li> <li><code>--img-size-sahi INT</code>: Image size for SAHI processing (default: 512)</li> <li><code>--overlap-sahi FLOAT</code>: Overlap ratio for SAHI (default: 0.2)</li> </ul>"},{"location":"cli-reference/#step-by-step-commands","title":"Step-by-Step Commands","text":""},{"location":"cli-reference/#step-1-frame-extraction","title":"Step 1: Frame Extraction","text":"<p>Extracts and aligns video frames to correct camera vibrations.</p> <pre><code>uv run python -m vivadatacreator.first_step --root /path/to/video.mp4\n</code></pre> <p>Parameters: - <code>--root PATH</code>: Input video file path (required)</p> <p>Outputs: - <code>imgsA/</code> folder with extracted frames - <code>video_alineado.mp4</code> aligned video</p>"},{"location":"cli-reference/#step-2-interactive-initial-segmentation","title":"Step 2: Interactive Initial Segmentation","text":"<p>Interactively segment objects in the first frame using SAM2.</p> <pre><code>uv run python -m vivadatacreator.second_step \\\n  --root /path/to/video.mp4 \\\n  --sam2-chkpt checkpoints/sam2.1_hiera_large.pt \\\n  --model-cfg /path/to/sam2/config.yaml\n</code></pre> <p>Parameters: - <code>--sam2-chkpt PATH</code>: SAM2 model checkpoint (required) - <code>--model-cfg PATH</code>: SAM2 configuration file (required)</p> <p>Outputs: - <code>mask_prompts.csv</code> with segmentation prompts</p>"},{"location":"cli-reference/#step-3-automatic-mask-propagation","title":"Step 3: Automatic Mask Propagation","text":"<p>Propagates initial masks throughout the video using SAM2 tracking.</p> <pre><code>uv run python -m vivadatacreator.third_step \\\n  --root /path/to/video.mp4 \\\n  --sam2-chkpt checkpoints/sam2.1_hiera_large.pt \\\n  --model-cfg /path/to/sam2/config.yaml \\\n  --n-imgs 200\n</code></pre> <p>Parameters: - <code>--n-imgs INT</code>: Batch size for processing</p> <p>Outputs: - <code>masks/</code> folder with individual mask files - <code>segmentation/</code> folder with combined masks per frame</p>"},{"location":"cli-reference/#step-4-object-detection-and-tracking","title":"Step 4: Object Detection and Tracking","text":"<p>Detects and tracks objects using YOLO and DeepSort.</p> <pre><code>uv run python -m vivadatacreator.fourth_step \\\n  --root /path/to/video.mp4 \\\n  --img-size-sahi 512 \\\n  --overlap-sahi 0.2\n</code></pre> <p>Optimized version with resource management: <pre><code>uv run python -m vivadatacreator.fourth_step_optimized \\\n  --root /path/to/video.mp4 \\\n  --auto-tune \\\n  --mask-cache auto \\\n  --ram-budget 0.6 \\\n  --device auto\n</code></pre></p> <p>Parameters: - <code>--device {auto|cuda|cpu}</code>: Execution device - <code>--batch-size INT</code>: Detection batch size - <code>--img-size-sahi INT</code>: SAHI slice size - <code>--overlap-sahi FLOAT</code>: SAHI overlap ratio - <code>--ram-budget FLOAT</code>: RAM usage limit (0.1-0.9) - <code>--mask-cache {auto|memory|disk}</code>: Caching strategy - <code>--yolo-weights PATH</code>: Custom YOLO weights - <code>--confidence-threshold FLOAT</code>: Detection confidence</p> <p>Outputs: - <code>track_dic.csv</code> with detected object tracks</p>"},{"location":"cli-reference/#step-5-interactive-mask-refinement","title":"Step 5: Interactive Mask Refinement","text":"<p>Refines detected objects with interactive SAM2 segmentation.</p> <pre><code>uv run python -m vivadatacreator.fifth_step \\\n  --root /path/to/video.mp4 \\\n  --sam2-chkpt checkpoints/sam2.1_hiera_large.pt \\\n  --model-cfg /path/to/sam2/config.yaml\n</code></pre> <p>Outputs: - <code>traked/</code> folder with refined masks - <code>mask_list.csv</code> with refined object list</p>"},{"location":"cli-reference/#step-6-enhanced-mask-propagation","title":"Step 6: Enhanced Mask Propagation","text":"<p>Propagates refined masks forward and backward through the video.</p> <pre><code>uv run python -m vivadatacreator.sixth_step \\\n  --root /path/to/video.mp4 \\\n  --sam2-chkpt checkpoints/sam2.1_hiera_large.pt \\\n  --model-cfg /path/to/sam2/config.yaml \\\n  --n-imgs 200\n</code></pre> <p>Outputs: - Updated <code>masks/</code> folder with enhanced segmentations</p>"},{"location":"cli-reference/#step-7-color-based-semantic-segmentation","title":"Step 7: Color-Based Semantic Segmentation","text":"<p>Creates color-coded semantic segmentation maps.</p> <pre><code>uv run python -m vivadatacreator.seventh_step --root /path/to/video.mp4\n</code></pre> <p>Outputs: - <code>semantic/</code> folder with colored segmentation images</p>"},{"location":"cli-reference/#step-8-final-dataset-creation","title":"Step 8: Final Dataset Creation","text":"<p>Combines images with semantic masks to create the final dataset.</p> <pre><code>uv run python -m vivadatacreator.eighth_step --root /path/to/video.mp4\n</code></pre> <p>Outputs: - <code>dataset/</code> folder with final dataset images</p>"},{"location":"cli-reference/#workflow-combinations","title":"Workflow Combinations","text":""},{"location":"cli-reference/#basic-sam2-only-workflow","title":"Basic SAM2-Only Workflow","text":"<p>For simple segmentation without YOLO detection:</p> <pre><code># Steps 1, 2, 3, 7, 8\nuv run python -m vivadatacreator.first_step --root video.mp4\nuv run python -m vivadatacreator.second_step --root video.mp4 --sam2-chkpt checkpoints/sam2.1_hiera_large.pt --model-cfg config.yaml\nuv run python -m vivadatacreator.third_step --root video.mp4 --sam2-chkpt checkpoints/sam2.1_hiera_large.pt --model-cfg config.yaml\nuv run python -m vivadatacreator.seventh_step --root video.mp4\nuv run python -m vivadatacreator.eighth_step --root video.mp4\n</code></pre>"},{"location":"cli-reference/#full-pipeline-with-refinement","title":"Full Pipeline with Refinement","text":"<p>Complete workflow including YOLO detection and refinement:</p> <pre><code># Steps 1-8\nuv run python -m vivadatacreator.first_step --root video.mp4\nuv run python -m vivadatacreator.second_step --root video.mp4 --sam2-chkpt checkpoints/sam2.1_hiera_large.pt --model-cfg config.yaml\nuv run python -m vivadatacreator.third_step --root video.mp4 --sam2-chkpt checkpoints/sam2.1_hiera_large.pt --model-cfg config.yaml\nuv run python -m vivadatacreator.fourth_step --root video.mp4\nuv run python -m vivadatacreator.fifth_step --root video.mp4 --sam2-chkpt checkpoints/sam2.1_hiera_large.pt --model-cfg config.yaml\nuv run python -m vivadatacreator.sixth_step --root video.mp4 --sam2-chkpt checkpoints/sam2.1_hiera_large.pt --model-cfg config.yaml\nuv run python -m vivadatacreator.seventh_step --root video.mp4\nuv run python -m vivadatacreator.eighth_step --root video.mp4\n</code></pre>"},{"location":"cli-reference/#quick-segmentation-skip-detection","title":"Quick Segmentation (Skip Detection)","text":"<p>For when you only need basic segmentation:</p> <pre><code># Steps 1, 2, 3, 7\nuv run python -m vivadatacreator.first_step --root video.mp4\nuv run python -m vivadatacreator.second_step --root video.mp4 --sam2-chkpt checkpoints/sam2.1_hiera_large.pt --model-cfg config.yaml\nuv run python -m vivadatacreator.third_step --root video.mp4 --sam2-chkpt checkpoints/sam2.1_hiera_large.pt --model-cfg config.yaml\nuv run python -m vivadatacreator.seventh_step --root video.mp4\n</code></pre>"},{"location":"cli-reference/#configuration-management","title":"Configuration Management","text":"<p>Commands automatically save settings to <code>config.yaml</code>. You can also manually manage configuration:</p> <pre><code># Download checkpoints manually\nuv run python -m vivadatacreator.download_checkpoints\n\n# Install additional dependencies\nuv run python -m vivadatacreator.install\n</code></pre>"},{"location":"cli-reference/#performance-tuning","title":"Performance Tuning","text":""},{"location":"cli-reference/#memory-management","title":"Memory Management","text":"<p>For large videos, adjust these parameters:</p> <pre><code># Reduce batch size for lower memory usage\n--n-imgs 50 --n-obj 10\n\n# Use disk caching for large datasets\n--mask-cache disk --ram-budget 0.3\n</code></pre>"},{"location":"cli-reference/#gpu-optimization","title":"GPU Optimization","text":"<pre><code># Force GPU usage\n--device cuda\n\n# Use optimized settings\nuv run python -m vivadatacreator.fourth_step_optimized --auto-tune\n</code></pre>"},{"location":"cli-reference/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli-reference/#common-issues","title":"Common Issues","text":"<p>CUDA out of memory: <pre><code># Reduce batch sizes\n--n-imgs 50 --batch-size 4 --ram-budget 0.5\n</code></pre></p> <p>Slow processing: <pre><code># Use optimized pipeline\nuv run python -m vivadatacreator.fourth_step_optimized --auto-tune\n</code></pre></p> <p>Missing checkpoints: <pre><code># Download manually\nuv run python -m vivadatacreator.download_checkpoints\n</code></pre></p>"},{"location":"installation/","title":"Installation","text":"<p>This guide provides instructions to install ViVa-SAFELAND.</p> <p>System Requirements</p> <p>The code automatically creates a Python 3.12 virtual environment using <code>uv</code>, even if a different Python version is installed on your system. </p>"},{"location":"installation/#1-setting-uv-the-python-project-manager","title":"1. Setting <code>UV</code>, the Python project manager","text":"<p>To facilitate the creation of virtual environments and manage Python packages and their dependencies we use a state of the art framework uv, its installation is straightforward and can be done via the following command:</p> macOS/LinuxWindows <p>Using <code>curl</code> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> Using <code>wget</code> <pre><code>wget -qO- https://astral.sh/uv/install.sh | sh\n</code></pre></p> <p>Use <code>irm</code> to download the script and execute it with <code>iex</code>: <pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre></p>"},{"location":"installation/#2-install-viva-datacreator","title":"2. Install ViVa-DataCreator","text":"<p>Choose one of the following installation methods:</p> From PyPIFrom Source <p>Recommended for most users</p> <p>Install the latest stable release from the Python Package Index (PyPI).</p> <pre><code>uv venv --python 3.12\nuv pip install viva-datacreator --upgrade\n</code></pre> <p>Recommended for developers</p> <p>Install the latest development version directly from the GitHub repository.</p> <pre><code>git clone https://github.com/viva-safeland/viva_datacreator.git\ncd viva_datacreator\nuv sync\n</code></pre> <p>Automatic Setup</p> <p>The application will automatically download the required SAM2 checkpoints on first launch if they are not already available. No manual download is required.</p>"},{"location":"installation/#4-video-requirements","title":"4. Video Requirements","text":"<p>To use ViVa-SAFELAND, you need video files for processing. The application accepts various video formats. Below the recommended specifications are listed:</p> <p>Video specifications:</p> <ul> <li>Format: MP4, AVI, MOV, MKV, WMV, FLV, WebM</li> <li>Resolution: 1080p or higher recommended for better segmentation quality</li> <li>Frame Rate: 30 FPS recommended</li> <li>Content: Videos containing objects you want to segment (people, vehicles, animals, etc.)</li> </ul>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide will walk you through a basic example of how to use ViVa-SAFELAND.</p> <p>First, make sure you have the necessary dependencies installed. Then, you can run the application directly from your terminal</p>"},{"location":"usage/#quick-start","title":"Quick Start","text":""},{"location":"usage/#graphical-user-interface-gui","title":"Graphical User Interface (GUI)","text":"<p>The primary way to use ViVa-SAFELAND is through its intuitive graphical user interface. Start it with:</p> <pre><code>uv run viva-creator\n</code></pre> <p>Automatic Setup</p> <p>The application will automatically download the required SAM2 checkpoints on first launch if they are not already available. Checkpoints are saved to a checkpoints/ directory that is automatically created at the root of the project.</p> <p>Example output when starting the GUI for the first time: <pre><code>$ uv run viva-creator\nUsing CPython 3.12.3 interpreter at: /usr/bin/python3.12\nCreating virtual environment at: .venv\nInstalled 91 packages in 468ms\nForest-dark theme loaded successfully\nOptimized defaults loaded\nSAM2 checkpoints not found. Downloading...\nDownloading sam2.1_hiera_tiny.pt to Dataset_Creator/checkpoints/sam2.1_hiera_tiny.pt...\nsam2.1_hiera_tiny.pt: 149MB [00:17, 9.12MB/s]                                                                              \nFinished downloading sam2.1_hiera_tiny.pt.\nDownloading sam2.1_hiera_small.pt to Dataset_Creator/checkpoints/sam2.1_hiera_small.pt...\nsam2.1_hiera_small.pt: 176MB [00:20, 8.97MB/s]                                                                             \nFinished downloading sam2.1_hiera_small.pt.\nDownloading sam2.1_hiera_base_plus.pt to Dataset_Creator/checkpoints/sam2.1_hiera_base_plus.pt...\nsam2.1_hiera_base_plus.pt: 309MB [00:39, 8.18MB/s]                                                                         \nFinished downloading sam2.1_hiera_base_plus.pt.\nDownloading sam2.1_hiera_large.pt to Dataset_Creator/checkpoints/sam2.1_hiera_large.pt...\nsam2.1_hiera_large.pt: 856MB [01:36, 9.33MB/s]                                                                             \nFinished downloading sam2.1_hiera_large.pt.\nDevice information: GPU: NVIDIA GeForce RTX 3060 (11.6 GB)\n</code></pre></p> ViVa-DataCreator: Graphical User Interface for Dataset Creation <p>The GUI provides:</p> <ul> <li>Process Selection Panel: Choose which step of the 8-step pipeline to execute</li> <li>Configuration Panel: Set all necessary parameters including video selection, SAM2 model configuration, and processing parameters</li> <li>Real-time Monitoring: View progress and status messages as processes run</li> <li>Automatic Configuration Saving: Your settings are saved and restored between sessions</li> </ul>"},{"location":"usage/#key-gui-features","title":"Key GUI Features","text":"<ul> <li>Video Selection: Browse and select your input video file</li> <li>Model Selection: Choose from available SAM2 models (checkpoints are downloaded automatically)</li> <li>Parameter Configuration:<ul> <li>Factor: Image scaling factor for processing efficiency</li> <li>Num Images: Batch size for frame processing</li> <li>Num Objects: Objects to process simultaneously</li> <li>SAHI Settings: Parameters for enhanced small object detection</li> </ul> </li> <li>Status Monitoring: Live updates on processing progress and device information</li> </ul>"},{"location":"usage/#configuration","title":"Configuration","text":"<p>The application automatically saves configuration to 'config.yaml', including:</p> <ul> <li>Video path</li> <li>SAM2 checkpoint path</li> <li>Model configuration path</li> <li>Processing parameters (factor, num_images, num_objects, etc.)</li> </ul> <p>This configuration is automatically loaded on subsequent runs.</p>"},{"location":"usage/#workflow-detailed-explanation","title":"Workflow: Detailed Explanation","text":"<p>The dataset creation process is divided into 8 steps that should be executed in order through the GUI.</p>"},{"location":"usage/#step-1-frame-extraction","title":"Step 1: Frame Extraction","text":"<ul> <li>Script: <code>first_step.py</code></li> <li>Purpose: Prepares the base material for the entire process. It extracts each frame from the video, aligns it with the previous frame to correct for small camera vibrations, and saves it as an image.</li> <li>Inputs:<ul> <li>The video file selected in the GUI (<code>--root</code>).</li> </ul> </li> <li>Outputs:<ul> <li><code>imgsA/</code> folder: Contains all the frames from the video as individual images (e.g., <code>00001.jpg</code>, <code>00002.jpg</code>, etc.).</li> <li><code>video_alineado.mp4</code> file: A new video created from the aligned frames. This video will be used in subsequent steps.</li> <li><code>static.png</code> file: The first frame of the video, saved as a static background image for use in later steps.</li> </ul> </li> </ul> Step 1: Left - Original Video, Right - Aligned Video"},{"location":"usage/#step-2-initial-interactive-segmentation","title":"Step 2: Initial Interactive Segmentation","text":"<ul> <li>Script: <code>second_step.py</code></li> <li>Purpose: In this step, you \"teach\" the model which objects you are interested in on the first frame of the video. This initial information is crucial for the model to be able to track these objects later.</li> <li>How it works:<ol> <li>A window will open showing the first frame of the video (resized according to the <code>Factor</code>).</li> <li>Click on the objects you want to segment. You will be prompted to enter <code>1</code> (positive click, to add this part to the object) or <code>0</code> (negative click, to exclude it).</li> <li>The SAM2 model will display the segmentation mask it is generating in real-time. You can add more points to refine it.</li> </ol> </li> <li>Controls:<ul> <li>Mouse Click: Adds a reference point (positive or negative).</li> <li><code>a</code> key (Add): When you are satisfied with the mask for an object, press <code>a</code>. A menu will open for you to select the object's class (e.g., \"car\", \"person\"). After selecting, you can start segmenting a new object.</li> <li><code>ESC</code> key (Escape): When you have finished segmenting all objects of interest in the frame, press <code>ESC</code>. You will be asked for the class of the last object, and the process will end.</li> </ul> </li> <li>Inputs:<ul> <li>The first frame from the <code>imgsA/</code> folder.</li> <li>Your interaction (clicks and class assignments).</li> </ul> </li> <li>Outputs:<ul> <li><code>mask_prompts.csv</code> file: A CSV file that saves the information for each object you segmented: the reference points, the labels (positive/negative), and the assigned class.</li> </ul> </li> </ul> Step 2: Left - Aligned Video, Right - Clicks and Masks (on cars)"},{"location":"usage/#step-3-mask-propagation-automatic-tracking","title":"Step 3: Mask Propagation (Automatic Tracking)","text":"<ul> <li>Script: <code>third_step.py</code></li> <li>Purpose: Using the information from <code>mask_prompts.csv</code>, this script processes the video in batches (<code>--n_imgs</code>) and propagates the initial masks through the frames, \"tracking\" the objects.</li> <li>How it works:<ol> <li>Loads the prompts from <code>mask_prompts.csv</code>.</li> <li>For the first batch of images, it creates the initial masks.</li> <li>For subsequent batches, it uses the mask from the last frame of the previous batch as the starting point for the current batch.</li> <li>Saves each generated mask as an individual image.</li> </ol> </li> <li>Inputs:<ul> <li><code>mask_prompts.csv</code>.</li> <li>Images from the <code>imgsA/</code> folder.</li> </ul> </li> <li>Outputs:<ul> <li><code>masks/</code> folder: Gets filled with thousands of mask images. Each file is named like <code>outmask_fr&lt;FRAME_NUM&gt;_id&lt;OBJECT_ID&gt;_cl&lt;CLASS&gt;.png</code>.</li> <li><code>segmentation/</code> folder: Contains images where all masks for a single frame are grouped into one image. The filename is the frame number (e.g., <code>1.png</code>, <code>2.png</code>).</li> </ul> </li> </ul> Step 3: Left - Objects Selected, Right - Individual Masks (cars)"},{"location":"usage/#step-4-detection-and-tracking-with-yolo-and-deepsort","title":"Step 4: Detection and Tracking with YOLO and DeepSort","text":"<ul> <li>Script: <code>fourth_step.py</code></li> <li>Purpose: This step temporarily ignores the SAM2 masks and performs object detection from scratch using a pre-trained YOLO model and the DeepSort tracker. The goal is to identify the most \"prominent\" or consistently moving objects, in order to refine their masks in the next step.</li> <li>How it works:<ol> <li>Processes the <code>video_alineado.mp4</code> frame by frame.</li> <li>In each frame, it applies an inverse mask (using masks from the <code>segmentation/</code> folder) to hide already segmented areas and avoid duplicate detections.</li> <li>Uses SAHI to improve the detection of small objects with YOLO.</li> <li>Uses DeepSort to assign a unique tracking ID to each detected object throughout the video.</li> </ol> </li> <li>Inputs:<ul> <li><code>video_alineado.mp4</code>.</li> <li>Masks from the <code>segmentation/</code> folder.</li> </ul> </li> <li>Outputs:<ul> <li><code>track_dic.csv</code> file: A CSV containing the <code>track_id</code> of each detected object and the first frame in which it appeared, along with its bounding box.</li> </ul> </li> </ul>"},{"location":"usage/#step-5-interactive-mask-refinement","title":"Step 5: Interactive Mask Refinement","text":"<ul> <li>Script: <code>fifth_step.py</code></li> <li>Purpose: To review the objects detected in Step 4 and create high-quality masks for them using SAM2 interactively. This is useful for adding objects that the tracking in Step 3 might have missed.</li> <li>How it works:<ol> <li>Reads the <code>track_dic.csv</code> file.</li> <li>For each object in the CSV, it shows you a crop (the bounding box) of the object.</li> <li>Just like in Step 2, you can use positive and negative clicks to generate a precise mask for that object.</li> </ol> </li> <li>Controls:<ul> <li>Mouse Click: Adds a reference point.</li> <li><code>a</code> key (Accept): Finishes adding points, prompts you to assign a class, and saves the mask.</li> <li><code>0</code> key (Skip): Skips the current object without generating a mask.</li> <li><code>ESC</code> key (Escape): Ends the entire Step 5 process.</li> </ul> </li> <li>Inputs:<ul> <li><code>track_dic.csv</code>.</li> <li>Images from the <code>imgsA/</code> folder.</li> </ul> </li> <li>Outputs:<ul> <li><code>traked/</code> folder: Saves the new, interactively generated masks.</li> <li><code>mask_list.csv</code> file: A list of all masks generated in this step, with their frame, class, and ID.</li> </ul> </li> </ul> Step 5: Interactive Mask Refinement"},{"location":"usage/#step-6-propagation-of-refined-masks","title":"Step 6: Propagation of Refined Masks","text":"<ul> <li>Script: <code>sixth_step.py</code></li> <li>Purpose: Similar to Step 3, but this time it uses the refined and additional masks created in Step 5 (<code>mask_list.csv</code>) and propagates them forward and backward through the video to complete the segmentation of those objects.</li> <li>How it works:<ol> <li>For each mask in <code>mask_list.csv</code>, it uses it as a starting point.</li> <li>It processes the video in batches (<code>--n_imgs</code>) forward from the mask's frame, generating and saving the segmentations.</li> <li>It then processes the video backward from the same frame to complete the segmentation in the preceding frames.</li> </ol> </li> <li>Inputs:<ul> <li><code>mask_list.csv</code>.</li> <li>Images from the <code>imgsA/</code> folder.</li> </ul> </li> <li>Outputs:<ul> <li><code>masks/</code> folder: Adds and/or overwrites the masks of the refined objects, ensuring complete segmentation throughout the entire video.</li> </ul> </li> </ul>"},{"location":"usage/#step-7-creation-of-semantic-color-masks","title":"Step 7: Creation of Semantic Color Masks","text":"<ul> <li>Script: <code>seventh_step.py</code></li> <li>Purpose: To unify all generated masks into a single image per frame, where each object class has a unique color. This creates a visual representation of the semantic segmentation.</li> <li>How it works:<ol> <li>Reads the class colors from the <code>class_dict.csv</code> file.</li> <li>For each frame, it combines all corresponding masks from the <code>masks/</code> folder.</li> <li>It paints each mask with the color of its class.</li> </ol> </li> <li>Inputs:<ul> <li>All masks in the <code>masks/</code> folder.</li> <li><code>class_dict.csv</code> for the colors.</li> </ul> </li> <li>Outputs:<ul> <li><code>semantic/</code> folder: Contains the semantic segmentation images, one for each frame, with objects colored according to their class.</li> </ul> </li> </ul> Left - Individual Masks, Right - Combined Masks (cars)"},{"location":"usage/#step-8-final-dataset-composition","title":"Step 8: Final Dataset Composition","text":"<ul> <li>Script: <code>eighth_step.py</code></li> <li>Purpose: To create the final dataset by combining the original images with the semantic segmentation masks.</li> <li>How it works:<ol> <li>Takes the first image of the video (<code>static.png</code>) as a background.</li> <li>For each image in the <code>semantic/</code> folder, it overlays the color masks onto the background image. The unsegmented (black) areas are replaced with the content from the static image.</li> </ol> </li> <li>Inputs:<ul> <li>Images from the <code>semantic/</code> folder.</li> <li>A static background image (the first frame of the video).</li> </ul> </li> <li>Outputs:1<ul> <li><code>dataset/</code> folder: The final dataset, with images showing the segmented and colored objects on a realistic background.</li> </ul> </li> </ul> Step 8: Final Dataset Composition"}]}